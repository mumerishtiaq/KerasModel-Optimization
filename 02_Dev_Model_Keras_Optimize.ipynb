{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498e0549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import s3fs\n",
    "import logging\n",
    "import sys\n",
    "import pickle \n",
    "import io\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "s3 = boto3.client('s3') \n",
    "s3FS = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bccccf",
   "metadata": {},
   "source": [
    "#### Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f351a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(file_path, bucket_name):\n",
    "    \"\"\"Creates file path in s3 bucket name or validates if exists\n",
    "    \n",
    "    Args:\n",
    "        file_path - string with directory name\n",
    "        bucket_name - string bucket\n",
    "    \"\"\"\n",
    "    # read s3\n",
    "    res = s3.list_objects_v2(\n",
    "        Bucket=bucket_name,\n",
    "        Prefix=file_path\n",
    "    )\n",
    "        \n",
    "    if not 'Contents' in res:\n",
    "        s3.put_object(Bucket=bucket_name, Key=(file_path+'/')) \n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def save_matrix(np_array, ids, bucket_name, key):\n",
    "    \"\"\"Saves tensors to bucket\n",
    "    \n",
    "    Args:\n",
    "        array - np array\n",
    "        ids - tensor with ids of members or items\n",
    "        bucket_name - string with bucket name\n",
    "        key - directory name + /model/ + filename.pkl\n",
    "    \"\"\"\n",
    "    # dump tensor to bytes\n",
    "    buffer = io.BytesIO()\n",
    "    matrix = pd.DataFrame(np_array, index = ids)\n",
    "    pickle.dump(matrix, buffer)\n",
    "    buffer.seek(0)\n",
    "    s3.upload_fileobj(buffer, bucket_name, key)   \n",
    "\n",
    "\n",
    "def get_pop(train,batch_size):\n",
    "    \"\"\"Calculates pop\n",
    "\n",
    "    Args:\n",
    "        train - tensordataframe\n",
    "    \"\"\"\n",
    "    pop = 0\n",
    "\n",
    "    for batch in tqdm(train.batch(batch_size)):\n",
    "        pop += tf.math.reduce_sum(tf.where(tf.math.greater(batch[:,:-1], 0), 1.0, 0.0), axis = 0 )\n",
    "\n",
    "    return pop\n",
    "\n",
    "\n",
    "class SerLogisticMF():\n",
    "\n",
    "    def __init__(self, save_path, bucket_name, F, num_users, num_items, popularity, lr, l2=1e-4, l2_bias=.001, alpha = .5):\n",
    "        self.F = F\n",
    "        self.learn_rate = lr\n",
    "        self.l2 = tf.keras.regularizers.l2(l2)\n",
    "        self.l2_bias = tf.keras.regularizers.l2(l2_bias)\n",
    "        self.alpha = alpha\n",
    "        self.m, self.n = (num_users, num_items)\n",
    "        self.popularity_pow = tf.pow(popularity, tf.Variable(self.alpha))\n",
    "        self.save_path = save_path\n",
    "        self.bucket_name = bucket_name\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "    \n",
    "    def difference(self,X):\n",
    "        n = tf.shape(X)[0]\n",
    "        m = tf.shape(X)[1]\n",
    "        X1 = tf.expand_dims(X, -1)\n",
    "        X2 = tf.reshape(X, (n, 1, m))\n",
    "        return tf.math.subtract(X1, X2)\n",
    "    \n",
    "  \n",
    "    def coherent(self,X):\n",
    "        n = tf.shape(X)[0]\n",
    "        m = tf.shape(X)[1]\n",
    "        X1 = tf.expand_dims(X, -1)\n",
    "        X2 = tf.reshape(X, (n, 1, m))\n",
    "        return tf.where(tf.math.greater(X1, X2), 1.0, 0.0)\n",
    "    \n",
    "    def auc_metric(self, batch, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate AUC\n",
    "        \"\"\"\n",
    "        coherent_pairs = self.coherent(batch[:,:-1])\n",
    "        auc = (\\\n",
    "                 self.coherent(y_pred)*\\\n",
    "                 coherent_pairs\n",
    "                ) \n",
    "\n",
    "        auc = tf.math.reduce_sum(auc, axis =1)\n",
    "        auc = tf.math.reduce_sum(auc, axis =1)\n",
    "        denumerator = tf.reduce_sum(tf.reduce_sum(coherent_pairs,axis=1),axis=1)\n",
    "        auc = tf.math.divide(auc, denumerator)\n",
    "\n",
    "        return auc\n",
    "    \n",
    "\n",
    "    def sauc_loss(self,batch,y_pred):\n",
    "        \"\"\"\n",
    "        Calculate loss\n",
    "\n",
    "        Args:\n",
    "          minibatch: minibatch size batchsize from ratings matrix\n",
    "        \"\"\"\n",
    "        #1. Calculate SAUC per member in batch\n",
    "        coherent_pairs = self.coherent(batch[:,:-1])\n",
    "        loss = (\\\n",
    "                 -tf.math.log_sigmoid(self.difference(y_pred))*\\\n",
    "                 coherent_pairs*\\\n",
    "                 self.popularity_pow\n",
    "                ) \n",
    "    \n",
    "        loss = tf.math.reduce_sum(loss, axis =1)\n",
    "        loss = tf.math.reduce_sum(loss, axis =1)\n",
    "        denumerator = tf.reduce_sum(tf.reduce_sum(coherent_pairs,axis=1),axis=1)\n",
    "        loss = tf.math.divide(loss, denumerator)\n",
    "        \n",
    "        #3. Calculate loss for batch\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return loss \n",
    "\n",
    "    def create_model(self):\n",
    "        inputs = tf.keras.layers.Input(shape=(self.n+1,), name=\"inputs\")\n",
    "        \n",
    "        user_embeddings = tf.keras.layers.Embedding(\n",
    "          input_dim=self.m, output_dim=self.F, name=\"user_embedding\",\n",
    "          embeddings_regularizer=self.l2,\n",
    "          embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1/np.sqrt(self.F)))(inputs[:,-1])\n",
    "        \n",
    "        items_range = tf.range(self.n, delta=1, dtype=tf.float32)\n",
    "        items_inputs = K.ones_like(inputs[:,:-1],dtype=tf.float32)*items_range\n",
    "        \n",
    "        item_embeddings = tf.keras.layers.Embedding(\n",
    "          input_dim=self.n, output_dim=self.F, name=\"item_embedding\",\n",
    "          embeddings_regularizer=self.l2,\n",
    "          embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1/np.sqrt(self.F)))(items_inputs)\n",
    "        \n",
    "        item_embeddings = tf.keras.layers.Permute((2, 1))(item_embeddings)\n",
    "        \n",
    "        dots = tf.keras.layers.Lambda(lambda x: K.batch_dot(x[0],x[1]))([user_embeddings, item_embeddings])\n",
    "        \n",
    "        user_biases = tf.keras.layers.Embedding(\n",
    "            input_dim=self.m, output_dim=1, name=\"user_bias\",\n",
    "            embeddings_regularizer=self.l2_bias)(inputs[:,-1])\n",
    "        \n",
    "        item_biases = tf.keras.layers.Embedding(\n",
    "            input_dim=self.n, output_dim=1, name=\"item_bias\",\n",
    "            embeddings_regularizer=self.l2_bias)(items_inputs)\n",
    "        \n",
    "        item_biases = K.squeeze(item_biases,2)\n",
    "        \n",
    "        dots = tf.keras.layers.Add()([dots, user_biases, item_biases])\n",
    "        \n",
    "        model = tf.keras.Model(\n",
    "          name=\"matrix_factorizer\",\n",
    "          inputs=[inputs], outputs=dots)\n",
    "        \n",
    "        model.add_loss(self.sauc_loss(inputs,dots))\n",
    "        model.add_metric(self.auc_metric(inputs,dots), name='auc_metric',aggregation='mean')\n",
    "        \n",
    "        model.compile(\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=self.learn_rate)\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, train, train_size, validation, val_size, epochs = 100, batch_size = 128, patience=5,path_name_log=\"../results/SerLMF/model_history_log.csv\"):\n",
    "        \n",
    "        train = train.shuffle(buffer_size=8*batch_size).batch(batch_size)\n",
    "        steps_per_epoch = train_size // batch_size\n",
    "        \n",
    "        \n",
    "        validation = validation.batch(batch_size)\n",
    "        validation_steps = val_size // batch_size\n",
    "        \n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_auc_metric', patience=patience, mode='max')\n",
    "        csv_logger = tf.keras.callbacks.CSVLogger(path_name_log, append=True)\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            train,\n",
    "            epochs = epochs,\n",
    "            validation_data = validation, \n",
    "            callbacks = [callback, csv_logger],\n",
    "            validation_steps = validation_steps,\n",
    "            use_multiprocessing = True,\n",
    "            max_queue_size = 512,\n",
    "            workers=8,\n",
    "            steps_per_epoch = steps_per_epoch\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def save(self, users_id,items_id): \n",
    "        \"\"\"Save matrix tensors to S3\"\"\"\n",
    "        # ensure save path exists\n",
    "        ensure_dir(self.save_path, self.bucket_name)\n",
    "    \n",
    "        keys = [self.save_path + '/model/P.pkl', self.save_path + '/model/Q.pkl', self.save_path + '/model/bias_u.pkl', self.save_path + '/model/bias_i.pkl' ]\n",
    "        matrices = [\n",
    "              self.model.get_weights()[1]   # User embedding\n",
    "            , self.model.get_weights()[0]   # Item embedding\n",
    "            , self.model.get_weights()[3]   # User bias embedding\n",
    "            , self.model.get_weights()[2]   # Item bias embedding\n",
    "\n",
    "        ]\n",
    "\n",
    "        ids = [users_id, items_id, users_id, items_id]\n",
    "        for matrix, idx, key in zip(matrices, ids, keys):\n",
    "            save_matrix(matrix, idx, self.bucket_name, key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06d7a8",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82bf1b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CREATE DUMMY DATA\n",
    "train_size = 10000\n",
    "val_size = 1000\n",
    "item_size = 1600\n",
    "\n",
    "train = tf.random.normal([train_size,item_size], 0, 1, tf.float32)\n",
    "ids = tf.reshape(tf.cast(tf.range(0, train_size, 1),tf.float32),(-1,1))\n",
    "train = tf.concat([train,ids] , axis=1)\n",
    "train = tf.data.Dataset.from_tensor_slices(train)\n",
    "\n",
    "validation = tf.random.normal([val_size,item_size], 0, 1, tf.float32)\n",
    "ids = tf.reshape(tf.cast(tf.range(0, val_size, 1),tf.float32),(-1,1))\n",
    "validation = tf.cast(tf.concat([validation,ids] , axis=1),tf.float32)\n",
    "validation = tf.data.Dataset.from_tensor_slices(validation)\n",
    "\n",
    "train_user_ids = np.arange(0,train_size)\n",
    "train_item_ids = np.arange(0,item_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba2acf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:00<00:00, 149.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# poppularity\n",
    "popularity =  get_pop(train,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c371e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "SerLMF = SerLogisticMF(save_path='',\n",
    "                       bucket_name=\"\", \n",
    "                       F=100, \n",
    "                       popularity = popularity, \n",
    "                       num_users=train_size,\n",
    "                       num_items=item_size, \n",
    "                       l2 = 1e-4,\n",
    "                       l2_bias = 1 ,   \n",
    "                       lr=.01 , \n",
    "                       alpha =.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b67d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8/19 [===========>..................] - ETA: 1:37 - loss: 52.6367 - auc_metric: 0.4999"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = SerLMF.fit(train, train_size, validation, val_size ,epochs = 5, batch_size = 512, patience=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
