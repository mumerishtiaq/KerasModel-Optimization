{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI_CarN2V_Ma",
        "outputId": "d970c710-8e32-4965-f60e-cf436be09bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "iI_CarN2V_Ma"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yKWkMaNwuuq",
        "outputId": "80eb5fb0-079e-4fff-f359-7ff01bddaec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.84-py3-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.7/134.7 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.84\n",
            "  Downloading botocore-1.29.84-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.84->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.84->boto3) (1.26.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.84->boto3) (1.15.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.84 botocore-1.29.84 jmespath-1.0.1 s3transfer-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting s3fs\n",
            "  Downloading s3fs-2023.3.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from s3fs) (3.8.4)\n",
            "Collecting fsspec==2023.3.0\n",
            "  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiobotocore~=2.4.2\n",
            "  Downloading aiobotocore-2.4.2-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.8/dist-packages (from aiobotocore~=2.4.2->s3fs) (1.15.0)\n",
            "Collecting aioitertools>=0.5.1\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.27.60,>=1.27.59\n",
            "  Downloading botocore-1.27.59-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.2)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.8/dist-packages (from aioitertools>=0.5.1->aiobotocore~=2.4.2->s3fs) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (2.8.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (1.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.8/dist-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (1.26.14)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (1.15.0)\n",
            "Installing collected packages: fsspec, aioitertools, botocore, aiobotocore, s3fs\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.1.0\n",
            "    Uninstalling fsspec-2023.1.0:\n",
            "      Successfully uninstalled fsspec-2023.1.0\n",
            "  Attempting uninstall: botocore\n",
            "    Found existing installation: botocore 1.29.84\n",
            "    Uninstalling botocore-1.29.84:\n",
            "      Successfully uninstalled botocore-1.29.84\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "boto3 1.26.84 requires botocore<1.30.0,>=1.29.84, but you have botocore 1.27.59 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiobotocore-2.4.2 aioitertools-0.11.0 botocore-1.27.59 fsspec-2023.3.0 s3fs-2023.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3\n",
        "!pip install s3fs"
      ],
      "id": "1yKWkMaNwuuq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "498e0549"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import s3fs\n",
        "import logging\n",
        "import sys\n",
        "import pickle \n",
        "import io\n",
        "import time\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "s3 = boto3.client('s3') \n",
        "s3FS = s3fs.S3FileSystem()"
      ],
      "id": "498e0549"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10bccccf"
      },
      "source": [
        "#### Keras Model"
      ],
      "id": "10bccccf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f351a24a"
      },
      "outputs": [],
      "source": [
        "def ensure_dir(file_path, bucket_name):\n",
        "    \"\"\"Creates file path in s3 bucket name or validates if exists\n",
        "    \n",
        "    Args:\n",
        "        file_path - string with directory name\n",
        "        bucket_name - string bucket\n",
        "    \"\"\"\n",
        "    # read s3\n",
        "    res = s3.list_objects_v2(\n",
        "        Bucket=bucket_name,\n",
        "        Prefix=file_path\n",
        "    )\n",
        "        \n",
        "    if not 'Contents' in res:\n",
        "        s3.put_object(Bucket=bucket_name, Key=(file_path+'/')) \n",
        "    \n",
        "    else:\n",
        "        pass\n",
        "\n",
        "def save_matrix(np_array, ids, bucket_name, key):\n",
        "    \"\"\"Saves tensors to bucket\n",
        "    \n",
        "    Args:\n",
        "        array - np array\n",
        "        ids - tensor with ids of members or items\n",
        "        bucket_name - string with bucket name\n",
        "        key - directory name + /model/ + filename.pkl\n",
        "    \"\"\"\n",
        "    # dump tensor to bytes\n",
        "    buffer = io.BytesIO()\n",
        "    matrix = pd.DataFrame(np_array, index = ids)\n",
        "    pickle.dump(matrix, buffer)\n",
        "    buffer.seek(0)\n",
        "    s3.upload_fileobj(buffer, bucket_name, key)   \n",
        "\n",
        "\n",
        "def get_pop(train,batch_size):\n",
        "    \"\"\"Calculates pop\n",
        "\n",
        "    Args:\n",
        "        train - tensordataframe\n",
        "    \"\"\"\n",
        "    pop = 0\n",
        "\n",
        "    for batch in tqdm(train.batch(batch_size)):\n",
        "        pop += tf.math.reduce_sum(tf.where(tf.math.greater(batch[:,:-1], 0), 1.0, 0.0), axis = 0 )\n",
        "\n",
        "    return pop\n",
        "\n",
        "\n",
        "class SerLogisticMF():\n",
        "\n",
        "    def __init__(self, save_path, bucket_name, F, num_users, num_items, popularity, learning_rate, l2=1e-4, l2_bias=.001, alpha = .5):\n",
        "        self.F = F\n",
        "        self.learn_rate = learning_rate\n",
        "        self.l2 = tf.keras.regularizers.l2(l2)\n",
        "        self.l2_bias = tf.keras.regularizers.l2(l2_bias)\n",
        "        self.alpha = alpha\n",
        "        self.m, self.n = (num_users, num_items)\n",
        "        self.popularity_pow = tf.pow(popularity, tf.Variable(self.alpha))\n",
        "        self.save_path = save_path\n",
        "        self.bucket_name = bucket_name\n",
        "        \n",
        "        self.model = self.create_model()\n",
        "    \n",
        "    def difference(self,X):\n",
        "        n = tf.shape(X)[0]\n",
        "        m = tf.shape(X)[1]\n",
        "        X1 = tf.expand_dims(X, -1)\n",
        "        X2 = tf.reshape(X, (n, 1, m))\n",
        "        return tf.math.subtract(X1, X2)\n",
        "    \n",
        "  \n",
        "    def coherent(self,X):\n",
        "        n = tf.shape(X)[0]\n",
        "        m = tf.shape(X)[1]\n",
        "        X1 = tf.expand_dims(X, -1)\n",
        "        X2 = tf.reshape(X, (n, 1, m))\n",
        "        return tf.where(tf.math.greater(X1, X2), 1.0, 0.0)\n",
        "    \n",
        "    def auc_metric(self, batch, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate AUC\n",
        "        \"\"\"\n",
        "        coherent_pairs = self.coherent(batch[:,:-1])\n",
        "        auc = (\\\n",
        "                 self.coherent(y_pred)*\\\n",
        "                 coherent_pairs\n",
        "                )\n",
        "        auc = tf.math.reduce_sum(auc, axis =1)\n",
        "        auc = tf.math.reduce_sum(auc, axis =1)\n",
        "        denumerator = tf.reduce_sum(tf.reduce_sum(coherent_pairs,axis=1),axis=1)\n",
        "        auc = tf.math.divide(auc, denumerator)\n",
        "\n",
        "        return auc\n",
        "\n",
        "    def sauc_loss(self,batch,y_pred):\n",
        "        \"\"\"\n",
        "        Calculate loss\n",
        "\n",
        "        Args:\n",
        "          minibatch: minibatch size batchsize from ratings matrix\n",
        "        \"\"\"\n",
        "        #1. Calculate SAUC per member in batch\n",
        "        coherent_pairs = self.coherent(batch[:,:-1])\n",
        "        loss = (\\\n",
        "                 -tf.math.log_sigmoid(self.difference(y_pred))*\\\n",
        "                 coherent_pairs*\\\n",
        "                 self.popularity_pow\n",
        "                ) \n",
        "    \n",
        "        loss = tf.math.reduce_sum(loss, axis =1)\n",
        "        loss = tf.math.reduce_sum(loss, axis =1)\n",
        "        denumerator = tf.reduce_sum(tf.reduce_sum(coherent_pairs,axis=1),axis=1)\n",
        "        loss = tf.math.divide(loss, denumerator)\n",
        "        \n",
        "        #3. Calculate loss for batch\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        return loss \n",
        "\n",
        "    def create_model(self):\n",
        "        inputs = tf.keras.layers.Input(shape=(self.n+1,), name=\"inputs\")\n",
        "        \n",
        "        user_embeddings = tf.keras.layers.Embedding(\n",
        "          input_dim=self.m, output_dim=self.F, name=\"user_embedding\",\n",
        "          embeddings_regularizer=self.l2,\n",
        "          embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1/np.sqrt(self.F)))(inputs[:,-1])\n",
        "        \n",
        "        items_range = tf.range(self.n, delta=1, dtype=tf.float32)\n",
        "        items_inputs = K.ones_like(inputs[:,:-1],dtype=tf.float32)*items_range\n",
        "        \n",
        "        item_embeddings = tf.keras.layers.Embedding(\n",
        "          input_dim=self.n, output_dim=self.F, name=\"item_embedding\",\n",
        "          embeddings_regularizer=self.l2,\n",
        "          embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1/np.sqrt(self.F)))(items_inputs)\n",
        "        \n",
        "        item_embeddings = tf.keras.layers.Permute((2, 1))(item_embeddings)\n",
        "        \n",
        "        dots = tf.keras.layers.Lambda(lambda x: K.batch_dot(x[0],x[1]))([user_embeddings, item_embeddings])\n",
        "        \n",
        "        user_biases = tf.keras.layers.Embedding(\n",
        "            input_dim=self.m, output_dim=1, name=\"user_bias\",\n",
        "            embeddings_regularizer=self.l2_bias)(inputs[:,-1])\n",
        "        \n",
        "        item_biases = tf.keras.layers.Embedding(\n",
        "            input_dim=self.n, output_dim=1, name=\"item_bias\",\n",
        "            embeddings_regularizer=self.l2_bias)(items_inputs)\n",
        "        \n",
        "        item_biases = K.squeeze(item_biases,2)\n",
        "        \n",
        "        dots = tf.keras.layers.Add()([dots, user_biases, item_biases])\n",
        "        \n",
        "        model = tf.keras.Model(\n",
        "          name=\"matrix_factorizer\",\n",
        "          inputs=[inputs], outputs=dots)\n",
        "        \n",
        "        model.add_loss(self.sauc_loss(inputs,dots))\n",
        "        model.add_metric(self.auc_metric(inputs,dots), name='auc_metric',aggregation='mean')\n",
        "\n",
        "        model.compile(\n",
        "          optimizer=tf.keras.optimizers.Adam(learning_rate=self.learn_rate)\n",
        "        )\n",
        "        return model\n",
        "    \n",
        "    def fit(self, train, train_size, validation, val_size, epochs = 100, batch_size = 128, patience=5,path_name_log=\"model_history_log.csv\"):#\"../results/SerLMF/model_history_log.csv\"):      \n",
        "        train = train.shuffle(buffer_size=8*batch_size).batch(batch_size)\n",
        "        steps_per_epoch = train_size // batch_size\n",
        "        \n",
        "        \n",
        "        validation = validation.batch(batch_size)\n",
        "        validation_steps = val_size // batch_size\n",
        "        \n",
        "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_auc_metric', patience=patience, mode='max')\n",
        "        csv_logger = tf.keras.callbacks.CSVLogger(path_name_log, append=True)\n",
        "        \n",
        "        history = self.model.fit(\n",
        "            train,\n",
        "            epochs = epochs,\n",
        "            validation_data = validation, \n",
        "            callbacks = [callback, csv_logger],\n",
        "            validation_steps = validation_steps,\n",
        "            use_multiprocessing = True,\n",
        "            max_queue_size = 512,\n",
        "            workers=8,\n",
        "            steps_per_epoch = steps_per_epoch,\n",
        "        )\n",
        "        \n",
        "        return history\n",
        "    \n",
        "    def save(self, users_id,items_id): \n",
        "        \"\"\"Save matrix tensors to S3\"\"\"\n",
        "        # ensure save path exists\n",
        "        ensure_dir(self.save_path, self.bucket_name)\n",
        "    \n",
        "        keys = [self.save_path + '/model/P.pkl', self.save_path + '/model/Q.pkl', self.save_path + '/model/bias_u.pkl', self.save_path + '/model/bias_i.pkl' ]\n",
        "        matrices = [\n",
        "              self.model.get_weights()[1]   # User embedding\n",
        "            , self.model.get_weights()[0]   # Item embedding\n",
        "            , self.model.get_weights()[3]   # User bias embedding\n",
        "            , self.model.get_weights()[2]   # Item bias embedding\n",
        "\n",
        "        ]\n",
        "\n",
        "        ids = [users_id, items_id, users_id, items_id]\n",
        "        for matrix, idx, key in zip(matrices, ids, keys):\n",
        "            save_matrix(matrix, idx, self.bucket_name, key)"
      ],
      "id": "f351a24a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82bf1b9e"
      },
      "outputs": [],
      "source": [
        "##### CREATE DUMMY DATA\n",
        "train_size = 100000\n",
        "val_size = 1000\n",
        "item_size = 1600\n",
        "epochs = 5\n",
        "\n",
        "train = tf.random.normal([train_size,item_size], 0, 1, tf.float32)\n",
        "ids = tf.reshape(tf.cast(tf.range(0, train_size, 1),tf.float32),(-1,1))\n",
        "train = tf.concat([train,ids] , axis=1)\n",
        "train = tf.data.Dataset.from_tensor_slices(train)\n",
        "train = train.repeat(epochs)\n",
        "\n",
        "validation = tf.random.normal([val_size,item_size], 0, 1, tf.float32)\n",
        "ids = tf.reshape(tf.cast(tf.range(0, val_size, 1),tf.float32),(-1,1))\n",
        "validation = tf.cast(tf.concat([validation,ids] , axis=1),tf.float32)\n",
        "validation = tf.data.Dataset.from_tensor_slices(validation)\n",
        "\n",
        "train_user_ids = np.arange(0,train_size)\n",
        "train_item_ids = np.arange(0,item_size)"
      ],
      "id": "82bf1b9e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba2acf87",
        "outputId": "cef48308-1f44-48b6-d7eb-047de0c336d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3907/3907 [00:08<00:00, 481.02it/s]\n"
          ]
        }
      ],
      "source": [
        "# poppularity\n",
        "popularity =  get_pop(train,128)"
      ],
      "id": "ba2acf87"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c371e7d"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "SerLMF = SerLogisticMF(save_path='',\n",
        "                       bucket_name=\"\", \n",
        "                       F=100, \n",
        "                       popularity = popularity, \n",
        "                       num_users=train_size,\n",
        "                       num_items=item_size, \n",
        "                       l2=1e-4,\n",
        "                       l2_bias=1 ,   \n",
        "                       learning_rate=.01 , \n",
        "                       alpha=.5)"
      ],
      "id": "6c371e7d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9b67d5e",
        "outputId": "cbe7caae-7c09-4388-8a4b-1ce85b71d1c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "# Train model\n",
        "history = SerLMF.fit(train, train_size, validation, val_size, epochs, batch_size=128, patience=2)\n",
        "end_time = time.time()"
      ],
      "id": "f9b67d5e"
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate time per step\n",
        "time_per_step = (end_time - start_time) / batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "7H3urq6lfkFZ",
        "outputId": "b050f80b-948e-40fd-a5f6-8b57a5bc6a33"
      },
      "id": "7H3urq6lfkFZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-5a24e72a4792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time per step for creating model: {end_time - start_time:.4f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GuHyhw40oPIB"
      },
      "id": "GuHyhw40oPIB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Dummy Dataset with consistancy**"
      ],
      "metadata": {
        "id": "Y3xBc1kp6rhF"
      },
      "id": "Y3xBc1kp6rhF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rp_l_qSTKyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f10955-069e-49b7-f489-3f424f8d9a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 1600)\n"
          ]
        }
      ],
      "source": [
        "train_size = 100000\n",
        "val_size = 1000\n",
        "item_size = 1600\n",
        "seed = 25\n",
        "epochs = 25\n",
        "\n",
        "# Create training data\n",
        "tf.random.set_seed(seed)\n",
        "train = tf.random.normal([train_size,item_size], 0, 1, tf.float32)\n",
        "print(train.shape)\n",
        "ids = tf.reshape(tf.cast(tf.range(0, train_size, 1),tf.float32),(-1,1))\n",
        "train = tf.concat([train,ids] , axis=1)\n",
        "train = tf.data.Dataset.from_tensor_slices(train)\n",
        "train = train.repeat(epochs)\n",
        "# Create validation data\n",
        "tf.random.set_seed(seed)\n",
        "validation = tf.random.normal([val_size,item_size], 0, 1, tf.float32)\n",
        "ids = tf.reshape(tf.cast(tf.range(0, val_size, 1),tf.float32),(-1,1))\n",
        "validation = tf.cast(tf.concat([validation,ids] , axis=1),tf.float32)\n",
        "validation = tf.data.Dataset.from_tensor_slices(validation)\n",
        "\n",
        "train_user_ids = np.arange(0,train_size)\n",
        "train_item_ids = np.arange(0,item_size)"
      ],
      "id": "_rp_l_qSTKyY"
    },
    {
      "cell_type": "code",
      "source": [
        "print(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bno3jsVcMSwh",
        "outputId": "6ad75f2f-da26-4346-d12f-f6c6da024bb3"
      },
      "id": "bno3jsVcMSwh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<RepeatDataset element_spec=TensorSpec(shape=(1601,), dtype=tf.float32, name=None)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC73ZrNxAJhz",
        "outputId": "d96372e7-5cae-401c-f145-6c0ea1848072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19532/19532 [00:35<00:00, 553.28it/s]\n"
          ]
        }
      ],
      "source": [
        "# poppularity\n",
        "popularity =  get_pop(train,128)\n",
        "# train\n",
        "SerLMF = SerLogisticMF(save_path='',\n",
        "                       bucket_name=\"\", \n",
        "                       F=100, \n",
        "                       popularity = popularity, \n",
        "                       num_users=train_size,\n",
        "                       num_items=item_size, \n",
        "                       l2=1e-4,\n",
        "                       l2_bias=1 ,   \n",
        "                       learning_rate=.0001 , \n",
        "                       alpha=.5)"
      ],
      "id": "hC73ZrNxAJhz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = SerLMF.fit(train, train_size, validation, val_size, epochs, batch_size=128, patience=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "J7sCFo1ATcb4",
        "outputId": "0f5b9a9a-e81b-48fe-e0e3-14fffce79b9e"
      },
      "id": "J7sCFo1ATcb4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "781/781 [==============================] - 171s 217ms/step - loss: 799.8876 - auc_metric: 0.5000 - val_loss: 782.8593 - val_auc_metric: 0.4996\n",
            "Epoch 2/25\n",
            "781/781 [==============================] - 170s 218ms/step - loss: 781.1240 - auc_metric: 0.5007 - val_loss: 781.3574 - val_auc_metric: 0.5003\n",
            "Epoch 3/25\n",
            "781/781 [==============================] - 170s 218ms/step - loss: 780.0398 - auc_metric: 0.5023 - val_loss: 780.2935 - val_auc_metric: 0.5014\n",
            "Epoch 4/25\n",
            "781/781 [==============================] - 170s 217ms/step - loss: 779.1730 - auc_metric: 0.5043 - val_loss: 779.4066 - val_auc_metric: 0.5028\n",
            "Epoch 5/25\n",
            "781/781 [==============================] - 170s 218ms/step - loss: 778.4434 - auc_metric: 0.5066 - val_loss: 778.6543 - val_auc_metric: 0.5045\n",
            "Epoch 6/25\n",
            "781/781 [==============================] - 170s 218ms/step - loss: 777.8148 - auc_metric: 0.5092 - val_loss: 778.0103 - val_auc_metric: 0.5064\n",
            "Epoch 7/25\n",
            "781/781 [==============================] - 170s 218ms/step - loss: 777.2696 - auc_metric: 0.5120 - val_loss: 777.4496 - val_auc_metric: 0.5085\n",
            "Epoch 8/25\n",
            "781/781 [==============================] - 170s 218ms/step - loss: 776.7870 - auc_metric: 0.5151 - val_loss: 776.9550 - val_auc_metric: 0.5109\n",
            "Epoch 9/25\n",
            "781/781 [==============================] - 170s 217ms/step - loss: 776.3539 - auc_metric: 0.5182 - val_loss: 776.5136 - val_auc_metric: 0.5134\n",
            "Epoch 10/25\n",
            "781/781 [==============================] - 170s 217ms/step - loss: 775.9578 - auc_metric: 0.5215 - val_loss: 776.1093 - val_auc_metric: 0.5160\n",
            "Epoch 11/25\n",
            "781/781 [==============================] - 169s 217ms/step - loss: 775.5859 - auc_metric: 0.5250 - val_loss: 775.7357 - val_auc_metric: 0.5188\n",
            "Epoch 12/25\n",
            "781/781 [==============================] - 170s 217ms/step - loss: 775.2323 - auc_metric: 0.5284 - val_loss: 775.3786 - val_auc_metric: 0.5217\n",
            "Epoch 13/25\n",
            "781/781 [==============================] - 170s 217ms/step - loss: 774.8855 - auc_metric: 0.5318 - val_loss: 775.0305 - val_auc_metric: 0.5246\n",
            "Epoch 14/25\n",
            "781/781 [==============================] - 170s 218ms/step - loss: 774.5384 - auc_metric: 0.5352 - val_loss: 774.6825 - val_auc_metric: 0.5276\n",
            "Epoch 15/25\n",
            "781/781 [==============================] - 170s 218ms/step - loss: 774.1849 - auc_metric: 0.5385 - val_loss: 774.3284 - val_auc_metric: 0.5306\n",
            "Epoch 16/25\n",
            "781/781 [==============================] - 170s 217ms/step - loss: 773.8198 - auc_metric: 0.5416 - val_loss: 773.9676 - val_auc_metric: 0.5335\n",
            "Epoch 17/25\n",
            "325/781 [===========>..................] - ETA: 1:38 - loss: 773.7252 - auc_metric: 0.5382"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f67e5f606132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerLMF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3d1a76d7508b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train, train_size, validation, val_size, epochs, batch_size, patience, path_name_log)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mcsv_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCSVLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_name_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         history = self.model.fit(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1654\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;31m# as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \"\"\"\n\u001b[1;32m   1154\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SerLMF.predict({\"user\": R_u, \"item\": R_i})"
      ],
      "metadata": {
        "id": "7JRocN2ZcCRH"
      },
      "id": "7JRocN2ZcCRH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gWnpA1PTcCiw"
      },
      "id": "gWnpA1PTcCiw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a dummy dataset of shape (num_samples, num_features)\n",
        "num_samples = 100000\n",
        "val_size = 1000\n",
        "num_items = 1600\n",
        "num_features = num_items + 1\n",
        "train_data = np.zeros((num_samples, num_features))\n",
        "train_data[:,:-1] = np.random.randint(0, num_items, (num_samples, num_items))\n",
        "train_data[:,-1] = np.random.randint(0, val_size, (num_samples,))\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
        "train_data = train_data[:,-1]\n",
        "\n",
        "validation = np.zeros((num_samples, num_features))\n",
        "validation[:,:-1] = np.random.randint(0, num_items, (num_samples, num_items))\n",
        "validation[:,-1] = np.random.randint(0, val_size, (num_samples,))\n",
        "validation = tf.data.Dataset.from_tensor_slices(validation)"
      ],
      "metadata": {
        "id": "jJRIDnPRTcvA"
      },
      "id": "jJRIDnPRTcvA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the popularity of each item\n",
        "popularity = get_pop(train_dataset, 128)"
      ],
      "metadata": {
        "id": "HaFekMqiT2O0"
      },
      "id": "HaFekMqiT2O0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "popularity.shape"
      ],
      "metadata": {
        "id": "0SLhb9KDVDbB"
      },
      "id": "0SLhb9KDVDbB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "SerLMF = SerLogisticMF(save_path='',\n",
        "                       bucket_name=\"\", \n",
        "                       F=100, \n",
        "                       popularity = popularity,\n",
        "                       num_users=num_samples,\n",
        "                       num_items=num_items, \n",
        "                       l2=1e-4,\n",
        "                       l2_bias=1 ,   \n",
        "                       learning_rate=.01 , \n",
        "                       alpha=.5)"
      ],
      "metadata": {
        "id": "Xx6r0fdzTkc8"
      },
      "id": "Xx6r0fdzTkc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = SerLMF.fit(train, num_samples, validation, val_size, epochs, batch_size=128, patience=2)"
      ],
      "metadata": {
        "id": "L1MP1QtfTpkj"
      },
      "id": "L1MP1QtfTpkj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfeRx4mBVre_"
      },
      "id": "SfeRx4mBVre_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "conda_amazonei_tensorflow2_p36",
      "language": "python",
      "name": "conda_amazonei_tensorflow2_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}